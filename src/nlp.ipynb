{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>pre_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from unidecode import unidecode\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(corpus):\n",
    "\n",
    "    # print('documento')\n",
    "\n",
    "    # print('#tokenizacao')\n",
    "    corpus_alt = re.findall(r\"\\w+(?:'\\w+)?|[^\\w\\s]\", corpus)\n",
    "\n",
    "    # print('#converte para minusculo')\n",
    "    corpus_alt = [t.lower() for t in corpus_alt]\n",
    "    \n",
    "    # print('#remove stopwords')\n",
    "    portuguese_stops = stopwords.words('portuguese')\n",
    "    corpus_alt = [t for t in corpus_alt if t not in portuguese_stops]\n",
    "    \n",
    "    # print('#remove numeros')\n",
    "    corpus_alt = [re.sub(r'\\d', '', t) for t in corpus_alt]\n",
    "    \n",
    "    # print('#remove pontuacao')\n",
    "    corpus_alt = [t for t in corpus_alt if t not in string.punctuation]\n",
    "    \n",
    "    # print('#remove acentos')\n",
    "    corpus_alt = [unidecode(t) for t in corpus_alt]\n",
    "\n",
    "    return corpus_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lorem', 'ipsum', 'dolor', 'sit', 'amet', 'consectetur', 'adipiscing', 'elit', 'sed', 'eiusmod', 'tempor', 'incididunt', 'ut', 'labore', 'et', 'dolore', 'magna', 'aliqua']\n"
     ]
    }
   ],
   "source": [
    "corpus_result = pre_processing(corpus)\n",
    "print(corpus_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>transform_to_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_sentences(corpus = '', break_sentense_by_char = '.'):\n",
    "    chunks = corpus.split(break_sentense_by_char)\n",
    "    chunks = list(filter(lambda x: x != '', chunks))\n",
    "    return [chunk + '.' for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lorem ipsum dolor sit amet, consectetur adipiscing elit.', ' Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.']\n"
     ]
    }
   ],
   "source": [
    "corpus_sentenses = transform_to_sentences(corpus)\n",
    "print(corpus_sentenses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>transform_to_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_chunks(corpus = '', chunk_size: int = 0):\n",
    "\n",
    "    chunks = []\n",
    "    words = corpus.split()\n",
    "    current_chunk = ''\n",
    "    \n",
    "    for word in words:\n",
    "        if len(current_chunk) + len(word) + 1 <= chunk_size:  # Add 1 to consider the space between the words\n",
    "            if current_chunk:\n",
    "                current_chunk += ' ' + word\n",
    "            else:\n",
    "                current_chunk = word\n",
    "        else:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = word\n",
    "    \n",
    "    if current_chunk:  # Add the last chunk if not empty\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'Lorem', 'ipsum', 'dolor', 'sit', 'amet,', 'consectetur', 'adipiscing', 'elit.', 'Sed', 'do', 'eiusmod', 'tempor', 'incididunt', 'ut', 'labore', 'et', 'dolore', 'magna', 'aliqua.']\n"
     ]
    }
   ],
   "source": [
    "print(transform_to_chunks(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lorem ipsum dolor sit amet,', 'consectetur adipiscing elit.', 'Sed do eiusmod tempor', 'incididunt ut labore et dolore', 'magna aliqua.']\n"
     ]
    }
   ],
   "source": [
    "print(transform_to_chunks(corpus, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>transform_text_in_chunks_interpolated_by_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text_in_chunks_interpolated_by_word(corpus = '', chunk_size: int = 0, n_word_interpolation: int = 0):\n",
    "\n",
    "    vect_words = pre_processing(corpus)\n",
    "\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "corpus_sentenses = transform_text_in_chunks_interpolated_by_word(corpus, 30, 1)\n",
    "print(corpus_sentenses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
